{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_basics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushagragpt99/NLP_practice/blob/master/nlp_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1UcTdoZLU2s",
        "colab_type": "code",
        "outputId": "9488aec9-925a-411d-bee4-388a3cafefe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2e1f018750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP30nK6gM7eJ",
        "colab_type": "code",
        "outputId": "6f8657cd-d267-4bee-f669-7e897d8b8a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "lin = nn.Linear(5, 3) # maps from R^5 to R^3, parameters A, b\n",
        "data = autograd.Variable( torch.randn(2, 5) ) # data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
        "print(lin(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0811, -0.3364, -0.1754],\n",
            "        [-0.4088,  0.6282,  0.7178]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BQdsAXpSSZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [ (\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "         (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "         (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "         (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\") ]\n",
        "\n",
        "test_data = [ (\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "              (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gve_HJwfb9L",
        "colab_type": "code",
        "outputId": "153710e7-517a-4566-f99f-984bf4a1f177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "word_ix={}\n",
        "for sentence,_ in data+test_data:\n",
        "  for word in sentence:\n",
        "    if(word not in word_ix):\n",
        "      word_ix[word]=len(word_ix)\n",
        "VOCAB_SIZE=len(word_ix)\n",
        "NO_LABEL=2\n",
        "print(word_ix)\n",
        "label_ix={}\n",
        "label_ix['SPANISH']=0\n",
        "label_ix['ENGLISH']=1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp8p7jKKhNnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BOWclassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, no_labels):\n",
        "      super(BOWclassifier, self).__init__()\n",
        "      self.linear=nn.Linear(vocab_size, no_labels)\n",
        "      \n",
        "    def forward(self, bow_vec):\n",
        "      return F.log_softmax(self.linear(bow_vec))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvk7FkUfg2Se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_bowvec(sent, word_ix):\n",
        "  bow_vec=torch.zeros(VOCAB_SIZE)\n",
        "  for word in sent:\n",
        "    bow_vec[word_ix[word]]+=1\n",
        "  return bow_vec.view(1,-1)\n",
        "  \n",
        "def make_target(word, label_ix):\n",
        "  return torch.LongTensor([label_ix[word]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKB5IBoRcSfR",
        "colab_type": "code",
        "outputId": "d3a3fa1b-ea1d-401e-ff2f-1dc076d91153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(label_ix)\n",
        "for x,y in data:\n",
        "    bow_vec=autograd.Variable(make_bowvec(x,word_ix))\n",
        "    label=autograd.Variable(make_target(y, label_ix))\n",
        "    print(label)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'SPANISH': 0, 'ENGLISH': 1}\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eA-DIifhJtM",
        "colab_type": "code",
        "outputId": "bc0344af-a9d3-4c53-c927-7e7c836ab82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "loss_fun=nn.NLLLoss()\n",
        "model=BOWclassifier(VOCAB_SIZE, NO_LABEL)\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "  for x,y in data:\n",
        "    bow_vec=autograd.Variable(make_bowvec(x,word_ix))\n",
        "    label=autograd.Variable(make_target(y, label_ix))\n",
        "    #bow_vec=make_bowvec(x,word_ix)\n",
        "    #label=make_target(y,label_ix)\n",
        "    y_hat=model(bow_vec)\n",
        "    #print(label)\n",
        "    loss=loss_fun(y_hat, label)\n",
        "    #print(loss)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-NOeQ1Dmhct",
        "colab_type": "code",
        "outputId": "ca0d6ab9-dd4d-4bee-ddd6-ec37c91c3e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "for x,y in test_data:\n",
        "    bow_vec=autograd.Variable(make_bowvec(x,word_ix))\n",
        "    p=model(bow_vec)\n",
        "    print(p)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.3284, -1.2731]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-1.2836, -0.3244]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhRo7_dxosTq",
        "colab_type": "code",
        "outputId": "e6439749-9643-4a10-fed3-f60b2f6d283d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(next(model.parameters())[:,word_ix[\"creo\"]])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.1262, -0.1731], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GguyhIT_pHeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [ ([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence) - 2) ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7HYDUwwdK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab=set(test_sentence)\n",
        "word_to_ix={word:i for i,word in enumerate(vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dqiIxGEwxvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed, context_size):\n",
        "    super(NGramLanguageModeler, self).__init__()\n",
        "    self.embedding=nn.Embedding(vocab_size, n_embed)\n",
        "    self.linear1=nn.Linear(context_size*n_embed, 128)\n",
        "    self.linear2=nn.Linear(128,vocab_size)\n",
        "    \n",
        "  def forward(self, words):\n",
        "    embed=self.embedding(words).view(1,-1)\n",
        "    out=F.relu(self.linear1(embed))\n",
        "    out=self.linear2(out)\n",
        "    return F.log_softmax(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJRAqmvC1DSY",
        "colab_type": "code",
        "outputId": "59401892-27c9-47b9-b658-10cb189ae136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "losses=[]\n",
        "model=NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "loss_fun=nn.NLLLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "  epoch_loss=torch.Tensor([0])\n",
        "  for context, target in trigrams:\n",
        "    context_idxs=map(lambda w: word_to_ix[w], context)\n",
        "    context_var=autograd.Variable(torch.LongTensor(list(context_idxs)))\n",
        "    #print(y_hat.size(), autograd.Variable(torch.LongTensor([word_to_ix[target]])))\n",
        "    y_hat=model(context_var)\n",
        "    loss=loss_fun(y_hat, autograd.Variable(torch.LongTensor([word_to_ix[target]])))\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss+=loss\n",
        "  losses.append(epoch_loss)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLRNMrBp5IWH",
        "colab_type": "code",
        "outputId": "6f88dbf4-b4c2-4d44-f651-20fc69870697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "CONTEXT_SIZE = 2 # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process. Computational processes are abstract\n",
        "beings that inhabit computers. As they evolve, processes manipulate other abstract\n",
        "things called data. The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "word_to_ix = { word: i for i, word in enumerate(set(raw_text)) }\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [ raw_text[i-2], raw_text[i-1], raw_text[i+1], raw_text[i+2] ]\n",
        "    target = raw_text[i]\n",
        "    data.append( (context, target) )\n",
        "print(data[:5])\n",
        "vocab_size=len(word_to_ix)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nYF-bM58stE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embed):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding=nn.Embedding(vocab_size, n_embed)\n",
        "    self.linear1=nn.Linear(n_embed, 128)\n",
        "    self.linear2=nn.Linear(128,vocab_size)\n",
        "    \n",
        "  def forward(self, input):\n",
        "    embed=torch.sum(self.embedding(input), dim=0)\n",
        "    out=F.relu(self.linear1(embed))\n",
        "    out=self.linear2(out)\n",
        "    return F.log_softmax(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB7c1KFMEj4Z",
        "colab_type": "code",
        "outputId": "e6c0bd46-217c-463b-b97e-8ca790a7fd1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = map(lambda w: word_to_ix[w], context)\n",
        "    tensor = torch.LongTensor(list(idxs))\n",
        "    return autograd.Variable(tensor)\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([32,  8,  7, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaUsDt6NEkZ0",
        "colab_type": "code",
        "outputId": "f3bb64c1-0cf2-4982-8db7-1a2e20cbd08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "for p in model.parameters():\n",
        "  print(p.size())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([97, 10])\n",
            "torch.Size([128, 20])\n",
            "torch.Size([128])\n",
            "torch.Size([97, 128])\n",
            "torch.Size([97])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35yhOKOjE5dY",
        "colab_type": "code",
        "outputId": "fb6039f9-0456-4f95-9193-d0e2663b4909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "model=CBOW(vocab_size, 10)\n",
        "loss_fun=nn.NLLLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "  for context, target in data:\n",
        "    con_vec=make_context_vector(context, word_to_ix)\n",
        "    y_hat=model(con_vec).view(1,-1)\n",
        "    #print(y_hat.size(), autograd.Variable(torch.LongTensor([word_to_ix[target]])))\n",
        "    loss=loss_fun(y_hat, autograd.Variable(torch.LongTensor([word_to_ix[target]])))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeyzDaMMdxn0",
        "colab_type": "code",
        "outputId": "9ce47cd9-c4ee-4e19-ed3c-beedc21292fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "a=torch.ones(3,4)\n",
        "em=nn.Embedding(3,4)\n",
        "b=autograd.Variable(torch.LongTensor([1,2]))\n",
        "print(b.size())\n",
        "c=torch.sum(em(b), dim=0)\n",
        "print(c)\n",
        "for p in em.parameters():\n",
        "  print(p.size())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2])\n",
            "tensor([ 0.5851,  0.6259,  2.0680, -1.3082], grad_fn=<SumBackward2>)\n",
            "torch.Size([3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvetWlTHDQ0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = nn.LSTM(3, 3) # Input dim is 3, output dim is 3\n",
        "inputs = [ autograd.Variable(torch.randn((1,3))) for _ in range(5) ] # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.  \n",
        "hidden = (autograd.Variable(torch.randn(1,1,3)), autograd.Variable(torch.randn((1,1,3))))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1,1,-1), hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ugnF6FA78ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1) # Add the extra 2nd dimension\n",
        "hidden = (autograd.Variable(torch.randn(1,1,3)), autograd.Variable(torch.randn((1,1,3)))) # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G9Eyu2l8VAH",
        "colab_type": "code",
        "outputId": "3f575440-117a-4d64-d12a-264a9e69bde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "out[4]==hidden[0]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 1, 1]]], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt-MehfABuV1",
        "colab_type": "code",
        "outputId": "f163307e-a5cf-4c88-9f4c-9b549b7c9abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "out.view(5,-1).size()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBwznxaW8W1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_sequence(seq, word_to_ix):\n",
        "  idxs=map(lambda w: word_to_ix[w], seq)\n",
        "  tensor=torch.LongTensor(list(idxs))\n",
        "  return autograd.Variable(tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLgvsARW8czH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "ix_to_word={}\n",
        "word_to_ix={}\n",
        "for sent,_ in training_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word]=len(word_to_ix)\n",
        "      ix_to_word[word_to_ix[word]]=word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztK6ddnS-edj",
        "colab_type": "code",
        "outputId": "57f46a89-9f3a-4dc8-fc31-39fd9e0f46d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 5"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GogdQfuh-lYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding=nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm=nn.LSTM(embedding_dim, hidden_dim)\n",
        "    self.hidden_2_tag=nn.Linear(hidden_dim, target_size)\n",
        "    self.hidden=self.init_hidden()\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (autograd.Variable(torch.zeros(1,1,self.hidden_dim)), \n",
        "            autograd.Variable(torch.zeros(1,1,self.hidden_dim)))\n",
        "  \n",
        "\n",
        "    \n",
        "  def forward(self, sentence):\n",
        "    embed=self.embedding(sentence)\n",
        "    lstm_out, self.hidden=self.lstm(embed.view(len(sentence), 1, -1), self.hidden)\n",
        "    #print('lstm_out size ', lstm_out.size())\n",
        "    tag_space=self.hidden_2_tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_score=F.log_softmax(tag_space)\n",
        "    return tag_score\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeY_2jXJmrIl",
        "colab_type": "code",
        "outputId": "ea83610f-f791-40b4-c614-4edc88dce8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embed=nn.Embedding(len(word_to_ix), EMBEDDING_DIM)\n",
        "sent=training_data[0][0]\n",
        "sent=make_sequence(sent, word_to_ix)\n",
        "out=embed(sent)\n",
        "out.size()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4VVBgD1CHmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYkhgGS3Cs3-",
        "colab_type": "code",
        "outputId": "9da50fe2-0ea1-45ae-d98b-8bf1f79989da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "for epoch in range(300):\n",
        "  for sent, target in training_data:\n",
        "    \n",
        "    seq=make_sequence(sent, word_to_ix)\n",
        "    y=make_sequence(target, tag_to_ix)\n",
        "    model.hidden=model.init_hidden()\n",
        "    y_hat=model(seq)\n",
        "    loss=loss_function(y_hat, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaw3OeeGE_lK",
        "colab_type": "code",
        "outputId": "83dcc406-804b-4317-bf92-38e223541ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "inputs = make_sequence(training_data[0][0], word_to_ix)\n",
        "tag_scores = model(inputs)\n",
        "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j for word i.\n",
        "# The predicted tag is the maximum scoring tag.\n",
        "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "# since 0 is index of the maximum value of row 1,\n",
        "# 1 is the index of maximum value of row 2, etc.\n",
        "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "print (tag_scores)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1030, -2.5107, -4.0929],\n",
            "        [-4.4713, -0.0430, -3.4834],\n",
            "        [-4.0574, -3.9167, -0.0379],\n",
            "        [-0.0436, -3.5922, -4.1914],\n",
            "        [-3.5066, -0.0385, -4.8591]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4miC0RI8GzDJ",
        "colab_type": "code",
        "outputId": "b63b3a66-78ce-43c7-ed0c-08ecfd1e0d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "list(training_data[0][0][0])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T', 'h', 'e']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1HGPAXhHH9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharLSTM(nn.Module):\n",
        "  def __init__(self, char_embed_size, word_embed_size, vocab_size, hidden_dim_char, hidden_dim_word, target_size_word):\n",
        "    super(CharLSTM, self).__init__()\n",
        "  \n",
        "    self.hidden_dim_char=hidden_dim_char\n",
        "    self.hidden_dim_word=hidden_dim_word\n",
        "  \n",
        "    self.embed_char=nn.Embedding(26*2, char_embed_size)\n",
        "    self.embed_word=nn.Embedding(vocab_size, word_embed_size)\n",
        "    self.lstm_char=nn.LSTM(char_embed_size, hidden_dim_char)\n",
        "    self.lstm_word=nn.LSTM(word_embed_size+hidden_dim_char, hidden_dim_word)\n",
        "    self.hidden2tag=nn.Linear(hidden_dim_word, target_size_word)\n",
        "    self.hidden_char=self.init_hidden_char()\n",
        "    self.hidden_word=self.init_hidden_word()\n",
        "  \n",
        "  def init_hidden_char(self):\n",
        "    return (autograd.Variable(torch.zeros(1,1,self.hidden_dim_char)),\n",
        "            autograd.Variable(torch.zeros(1,1,self.hidden_dim_char)))\n",
        "  \n",
        "  def init_hidden_word(self):\n",
        "    return (autograd.Variable(torch.zeros(1,1,self.hidden_dim_word)),\n",
        "            autograd.Variable(torch.zeros(1,1,self.hidden_dim_word)))\n",
        "  \n",
        "  def forward(self, seq, sentenc):\n",
        "#     char_list=list(map(lambda w: self.embed_char(make_char_seq(lambda q: split(q) for q in w, char_to_ix)), sentenc))\n",
        "#     char_tensor=torch.Tensor()\n",
        "#     for i in char_list:\n",
        "#       char_tensor=torch.cat((char_tensor,i),0)\n",
        "#     print(char_tensor.size())\n",
        "    a=torch.Tensor()\n",
        "    for val in seq:\n",
        "      word=ix_to_word[val.item()]\n",
        "      embed_char=self.embed_char(make_char_seq(word, char_to_ix))\n",
        "      lstm_out_char, self.hidden_char=self.lstm_char(embed_char.view(len(word),1,-1), self.hidden_char)\n",
        "      a=torch.cat((a,lstm_out_char[-1]))\n",
        "      \n",
        "    embed_word=self.embed_word(seq)\n",
        "    embed=torch.cat((embed_word, autograd.Variable(a)), 1).view(len(embed_word), 1, -1)\n",
        "    lstm_out, self.hidden_word=self.lstm_word(embed.view(len(seq), 1, -1), self.hidden_word)\n",
        "    #print(lstm_out.size())\n",
        "    tag_space=self.hidden2tag(lstm_out.view(len(seq), -1))\n",
        "    tag_score=F.log_softmax(tag_space)\n",
        "    return tag_score\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5wriTrxpNK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_ix={}\n",
        "for i in range(26):\n",
        "  char_to_ix[chr(65+i)]=i\n",
        "  \n",
        "for i in range(26):\n",
        "  char_to_ix[chr(97+i)]=i+26"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvVJbYfvhhQZ",
        "colab_type": "code",
        "outputId": "d10ae463-fcbb-4773-a171-00a01cb0c17c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embed_char=nn.Embedding(len(char_to_ix), EMBEDDING_DIM)\n",
        "l1=nn.Linear(EMBEDDING_DIM,3)\n",
        "l1(embed_char(make_char_seq('dog', char_to_ix))).size()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riwI6rf0qqzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_char_seq(word, char_to_ix):\n",
        "  idxs=map(lambda w: char_to_ix[w], word)\n",
        "  return autograd.Variable(torch.LongTensor(list(idxs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF2eS_DgvHP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAR_REP_SIZE=3\n",
        "model=CharLSTM(EMBEDDING_DIM, EMBEDDING_DIM, len(word_to_ix), CHAR_REP_SIZE, HIDDEN_DIM, len(tag_to_ix))\n",
        "loss_fun=nn.NLLLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTDs8SwP2tSs",
        "colab_type": "code",
        "outputId": "caff0c56-73ca-4bb9-a115-400f50341b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "for epoch in range(1):\n",
        "  for sent, target in training_data:\n",
        "    seq=make_sequence(sent, word_to_ix)\n",
        "    y=make_sequence(target, tag_to_ix)\n",
        "    model.hidden_char=model.init_hidden_char()\n",
        "    model.hidden_word=model.init_hidden_word()\n",
        "    \n",
        "    y_hat=model(seq, sent)\n",
        "    loss=loss_fun(y_hat, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Q_Gl4O8Ujn",
        "colab_type": "code",
        "outputId": "64ea8e1b-d1b4-4427-938a-bdf29f2ffbbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "inputs = make_sequence(training_data[0][0], word_to_ix)\n",
        "tag_scores = model(inputs, training_data[0][0])\n",
        "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j for word i.\n",
        "# The predicted tag is the maximum scoring tag.\n",
        "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "# since 0 is index of the maximum value of row 1,\n",
        "# 1 is the index of maximum value of row 2, etc.\n",
        "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "print (tag_scores)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.1920, -0.9908, -1.1236],\n",
            "        [-1.2085, -0.9848, -1.1152],\n",
            "        [-1.2323, -0.9698, -1.1111],\n",
            "        [-1.1970, -0.9678, -1.1457],\n",
            "        [-1.2016, -0.9416, -1.1736]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkbR7bYMCVz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}